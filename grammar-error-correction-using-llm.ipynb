{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Grammar Error Correction Using LLM (Large Language Models)\n\n## Introduction to Grammar Error Correction\n\nGrammar error correction (GEC) refers to the process of detecting and correcting grammatical errors in written text. This includes a wide range of mistakes, such as verb tense errors, subject-verb agreement, incorrect word order, preposition mistakes, and punctuation errors. Effective GEC is crucial for improving the readability, understanding, and professionalism of written communication. It plays a significant role in various applications such as content creation, learning English as a second language, business communications, and academic writing.\n\nThe importance of grammar error correction stems from its ability to ensure clarity and precision in communication. By reducing the incidence of errors, GEC helps maintain the author's credibility and ensures that the message is conveyed effectively without misunderstandings. Additionally, automated grammar correction tools empower individuals by saving time and effort that would otherwise be spent in manual proofreading and editing.","metadata":{}},{"cell_type":"markdown","source":"# Project Overview\n\nIn this project, titled \"Grammar Error Correction Using LLM,\" I will leverage the capabilities of Large Language Models (LLMs) to fine-tune a model specifically for the task of correcting grammatical errors in English text. Using state-of-the-art techniques in machine learning and natural language processing, this project aims to develop a robust system that can automatically correct a wide array of grammatical mistakes with high accuracy.","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning the Model\n\nTo achieve this, the project involves fine-tuning a pre-trained model on a large dataset of grammatically incorrect sentences paired with their corrected versions. The choice of model for this task is the T5 (Text-to-Text Transfer Transformer) model, known for its effectiveness in handling diverse text-based tasks through a unified text-to-text approach.","metadata":{}},{"cell_type":"markdown","source":"## Detailed Breakdown of the Project Components\n\n1. **Data Preprocessing**:\n    *  I Have used the data of lang-8 then i have preprocess the data on my local machine\n    * The dataset comprises approximately 5 million rows of data, each containing a pair of 'processed_input' (incorrect grammar) and 'processed_output' (corrected grammar).\n    * The data is split into training and validation sets, with the training set used to fine-tune the model and the validation set to evaluate its performance.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\ndf = pd.read_csv('/kaggle/input/gramer-checker/processed_data.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:19:24.637583Z","iopub.execute_input":"2024-04-16T11:19:24.638405Z","iopub.status.idle":"2024-04-16T11:19:26.345835Z","shell.execute_reply.started":"2024-04-16T11:19:24.638372Z","shell.execute_reply":"2024-04-16T11:19:26.345030Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:19:50.164822Z","iopub.execute_input":"2024-04-16T11:19:50.165606Z","iopub.status.idle":"2024-04-16T11:19:50.297088Z","shell.execute_reply.started":"2024-04-16T11:19:50.165575Z","shell.execute_reply":"2024-04-16T11:19:50.296312Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.1)\n \n# Convert dataframes into Hugging Face dataset objects\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:20:02.352823Z","iopub.execute_input":"2024-04-16T11:20:02.353620Z","iopub.status.idle":"2024-04-16T11:20:03.006260Z","shell.execute_reply.started":"2024-04-16T11:20:02.353588Z","shell.execute_reply":"2024-04-16T11:20:03.005485Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Model Selection and Tokenization**:\n   * The T5 model is chosen for its versatility and ability to generate text based on the context provided in the input.\n   * The T5Tokenizer is used to convert text into tokens that serve as input for the model. This involves encoding the text sequences and setting up the model to interpret these sequences correctly.","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\n\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples['processed_input'], max_length=128, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples['processed_output'], max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:20:16.320021Z","iopub.execute_input":"2024-04-16T11:20:16.320827Z","iopub.status.idle":"2024-04-16T11:23:03.239755Z","shell.execute_reply.started":"2024-04-16T11:20:16.320786Z","shell.execute_reply":"2024-04-16T11:23:03.238836Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaad8968c2cc41eb81e4485474d7de0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d58ea26a58648a38ac69865abef0d97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f707a7f6be894c1f9bd91012b25630d9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/453477 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdaa126ef61d4e2cae459ceaf7550d6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf9cf15ce0a413f89d17a61354efc52"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\n\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:23:10.893052Z","iopub.execute_input":"2024-04-16T11:23:10.893689Z","iopub.status.idle":"2024-04-16T11:23:19.166219Z","shell.execute_reply.started":"2024-04-16T11:23:10.893655Z","shell.execute_reply":"2024-04-16T11:23:19.165379Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8a401ef262f47679f1eef9f1112ff1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b860112c9e5b4d7b9edd4081c1058f3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba540590b9424581a4ec5bc691f45187"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Training the Model:\n\n   * The model is trained using the Hugging Face Trainer API, which simplifies the training process with powerful GPU acceleration and efficient memory management.\n   * Training parameters such as batch size, number of epochs, and learning rate are configured to optimize the learning process.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=3,              # number of training epochs\n    per_device_train_batch_size=16,  # batch size for training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T11:27:35.381700Z","iopub.execute_input":"2024-04-16T11:27:35.382075Z","iopub.status.idle":"2024-04-16T15:36:53.549887Z","shell.execute_reply.started":"2024-04-16T11:27:35.382046Z","shell.execute_reply":"2024-04-16T15:36:53.548843Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='85029' max='85029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85029/85029 4:09:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.101900</td>\n      <td>0.085451</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.094400</td>\n      <td>0.083351</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.102100</td>\n      <td>0.082894</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=85029, training_loss=0.09249421293084507, metrics={'train_runtime': 14957.6744, 'train_samples_per_second': 90.952, 'train_steps_per_second': 5.685, 'total_flos': 4.603079557958861e+16, 'train_loss': 0.09249421293084507, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the trained model and tokenizer\nmodel.save_pretrained('./saved_model')\ntokenizer.save_pretrained('./saved_tokenizer')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:39:52.624440Z","iopub.execute_input":"2024-04-16T15:39:52.624799Z","iopub.status.idle":"2024-04-16T15:39:53.081562Z","shell.execute_reply.started":"2024-04-16T15:39:52.624771Z","shell.execute_reply":"2024-04-16T15:39:53.080482Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('./saved_tokenizer/tokenizer_config.json',\n './saved_tokenizer/special_tokens_map.json',\n './saved_tokenizer/spiece.model',\n './saved_tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Evaluation and Correction Function:\n\n   * After training, the model's ability to correct new sentences is evaluated on the validation dataset to ensure that it generalizes well to unseen data.\n   * A function correct_grammar is created to take an input sentence, process it through the model, and output the corrected version of the sentence.","metadata":{}},{"cell_type":"code","source":"# Load the trained model and tokenizer\nmodel = T5ForConditionalGeneration.from_pretrained('/kaggle/working/saved_model')\ntokenizer = T5Tokenizer.from_pretrained('/kaggle/working/saved_tokenizer')\n\ndef correct_grammar(sentence):\n    # Prefixing the input text with 'grammar:' is optional and depends on how the model was trained.\n    # If you used a specific prefix during training (like \"grammar correction:\"), use the same here.\n    inputs = tokenizer.encode(\"grammar: \" + sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n    \n    # Generate output using the model\n    outputs = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    \n    # Decode the generated ids to a string\n    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return corrected_sentence\n\n# Test the function\nsentence = \"i was watched tv then my father has call me for do her work\"\ncorrected_sentence = correct_grammar(sentence)\nprint(\"Original:\", sentence)\nprint(\"Corrected:\", corrected_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:52:23.067461Z","iopub.execute_input":"2024-04-16T15:52:23.068150Z","iopub.status.idle":"2024-04-16T15:52:24.146764Z","shell.execute_reply.started":"2024-04-16T15:52:23.068118Z","shell.execute_reply":"2024-04-16T15:52:24.145744Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Original: i was watched tv then my father has call me for do her work\nCorrected: grammar: i watched tv then my father called me to do her work\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\nimport os\n\ndef zip_directory(folder_path, output_path):\n    \"\"\"Zips the contents of an entire directory.\"\"\"\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create a proper archive path by getting the path relative to the folder to be zipped\n                archive_path = os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..'))\n                zipf.write(os.path.join(root, file), arcname=archive_path)\n    print(f\"Created zip file: {output_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:43:26.664381Z","iopub.execute_input":"2024-04-16T15:43:26.665032Z","iopub.status.idle":"2024-04-16T15:43:26.672702Z","shell.execute_reply.started":"2024-04-16T15:43:26.665003Z","shell.execute_reply":"2024-04-16T15:43:26.671757Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Path to the directory where the model is saved\nmodel_directory = '/kaggle/working/saved_tokenizer'\n\n# Path to the output zip file\nzip_output_path = '/kaggle/working/tokenize.zip'\n\n# Call the function\nzip_directory(model_directory, zip_output_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:44:30.164589Z","iopub.execute_input":"2024-04-16T15:44:30.165436Z","iopub.status.idle":"2024-04-16T15:44:30.240872Z","shell.execute_reply.started":"2024-04-16T15:44:30.165404Z","shell.execute_reply":"2024-04-16T15:44:30.239860Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Created zip file: /kaggle/working/tokenize.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}